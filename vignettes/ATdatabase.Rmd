---
title: "ATdatabase"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ATdatabase}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(dplyr)
library(RSQLite)
library(pool)
library(ATdatabase)
```


# Introduction

The ATDatabase pacakge provides a caching database for the Analyse Together
(AT) Tool. With this tool people can analyse the data from their own
environmental sensor, like citizens science air quality sensors. The
AT tool follows a modular design so that we can easily adapt the tool
for different projects or different user groups.  This effectively
will result in one generic tool and some other versions, more catered towards
specific usages.

Data from a single environmental sensor are usually send to data
providers who store the data and make the data available using API's
or websites. Data exploration tools like the AT tool also combine data
from several providers, for example sensor data from community portals,
wheater data from meteorolical institutes and reference data from national
monitoring networks. All these data can be accessed by API's but
collecting the data is often slow. So it helps to reuse data after it
is downloaded from the API.

With this package you can create a database in which all the data from
different sources can be downloaded and stored in a generalised way for applications
like the AT tool. The database provides caching functions, so when
data is downloaded it is immediatly available for other users (or even
applications)

While the sensor data is stored using a generalised data model, the
pacakge also provides in data base tables and methods to store any
other (unstructured) data. For example, information about type of
sensor of about the location can also stored using any format, as long
it is usable within R.

# Creating the database

* database model (data + meta)
* SQLite is current backend  
* create and drop tables

## database model

The database model is based on the concept of mobile or static
stations. Each station contains equipment or sensors which measure a
certain parameter, and the values of these parameters change over time.

A station can be any assemblage of measuring equipment. For example it
can be a huge and expensive reference air monitoring station, but it
can also be a cheap hobyist ducktape assemblage of exotic electronics.
Each station contains one or more sensors.
Such sensor doesn't have to be an air quality monitoring
sensor, also meteorological sensors are possible. It can be anything
that measures something, somewhere, at some moment in time.

The variation of the measured values can be aggregated. If the sensor
measures a value each second, these values can be aggregated by
reporting the average of these values over one hour. For example, air
quality data is often reported as one hour averaged values, because
this gives a more robust and consistent result compared to the strong varying
real time data. It is up to the user to determine at which aggregation
level the data is stored in the database, the minimum is one value
each second.

While basic sensor information, like station name and position, and
measurements are stored as structured data, meta data about the station
or sensor (or project or region where they belong to) can be stored in any
format, as unstructured data.  For the storage of these meta data a NoSQL
approach is used.  In this approach meta data is stored as JSON documents.

The database uses four tables to store both measurements, caching
information and meta data. These four tables are:

* Measurements
* caching
* location  
* meta  

In the next sections we will explain the information model of the
database in detail.

### Measurements table and timestamps


The measurements table contains all the measurements

Table            | fields           | description
-----------------|------------------|-------------
measurements     | id               | primary key
.                | station          | station id 
.                | parameter        | parameter / sensor
.                | value            | measured value
.                | aggregation      | aggregation period in seconds
.                | timestamp        | date - time of observation


The station id is the name of the station, it is stored as a string.
Any name can be chosen. The parameter is the name of the measured
parameter stored as string. Addition information about the parameter,
like unit of sensor type, should be stored in the meta data.

The value, stored as 'real', is a numerical value of the measurement.
The aggregation field gives the aggregation level in seconds on which
the value is based on. 

Please note that you can not store values from the same station with
different aggregation levels. For example with air quality data, you
either have to chose between real time data or one hour averages. If
you want to use both, then collect the real time data and calculate
the one hour averages yourself.

The timestamp is the date and time of the observation. This is stored
in seconds! While odd at first to have these timestamps in seconds,
it is very practical to determine which time ranges are present in the
database and which time ranges should be added (this is what the
caching part of this package does). Seconds are also used internally
in the operating system, the 'start of time' (second 0) is the 'UNIX
epoch': 01-01-1970 at 00:00 UTC. The way we record timestamps is for
each table the same, so all timestamps in other tables are also in
seconds.

### Location table

The location table contains basic information about the station, like
name and position.

Table            | fields           | description
-----------------|------------------|-------------
location         | id               | primary key
.                | station          | station id
.                | lat, lon         | latitude / longitude
.                | timestamp        | date - time of station info


The station id is refered by the measurements table and contains the
name of the station. The lat and lon fields contain the position in latitude and
longitude of the station. While we asume that you store this position
using the WGS84 coordinate system, any coordinate system can be used,
even grid based systems, as long as you name the coordinates lat and
lon.

The timestamp contains the date and time when the position of the
station is recorded. Static stations have a single record in this
table while mobile station can have multiple records. It is up to the
user to combine station mobility with measurements.

### Caching table

In the caching table the time ranges for which information about a
station is present in the database, is recorded.

 
Table            | fields           | description
-----------------|------------------|-------------------------
caching          | id               | primary key
.                | station          | station id
.                | start, end       | time range / timestamps


Again, station is refered by the other tables. Start and end are the
start and end of a time range for which measurements are available in
the measurements table.

### Meta table

The meta table contains unstructured data about stations, sensors, or
whatever the user likes to store. 


Table            | fields           | description
---------------- | ---------------- | ------------
meta             | id               | primary key
.                | type             | type
.                | ref              | reference
.                | doc              | JSON doc


The type field determines the type of the meta data. For example
'station' for station info or 'project' for project info. It is up to
the user to define the type. The ref field is a reference. For example
if the type is 'station' the ref field can contain a station id to
refer to a station.

The doc field contains the meta data itself.  The user can store vectors, lists or
data.frames as meta data. Internally this meta data is converted to a
JSON object and stored in the doc field. When the data is retrieved
the user gets the data as R object (vector, list, etc).

Because meta data is stored as, e.g., a data.frame, it is easy to get
all the data of a certain type returned as single data.frames. These
data.frames can be combined, resulting in one large table. This table
can then be used further within the model or application.

While this looks complicated, it is actually a very versatile way to
store data as you will see in the examples further on in this
vignette.


## Create the database

The current version of this packages uses SQLite as database backend.
While we foresee that other database systems, like PostgreSQL, can be
used, it is not tested yet.

To create a database we have to create an SQLite database, these
databases are file based. For this vignette we create this database in
the tempory R directory.

```{r create_database1}

fname_db <- tempfile()
dbconn <- pool::dbPool(drv = RSQLite::SQLite(), dbname = fname_db)

```

After creating the database we have to create the database tables.
We can use the `create_database_tables` for that. After the
creation we check if the tables exists

```{r create_database2}

create_database_tables(dbconn)
pool::dbListTables(dbconn)

```

# Downloading and storing data

Before you can download and store (cache) your downloaded data you
have to make a few preparations. In general you have to create
functions to get the data about stations, for the station info table.
Also, you have to create a download function to obtain the station
measurements. These functions have to return the data in a specifific
format so they can be used in conjunction with the functions from this
package. For each data provider, you have to write these functions
since they will be specific for the data provider's API.

This package contains example data and we simulate the download of
data b reading this example data. It is not the real thing but i
should give you understanding how this all works. The sample data is a
single file with all the information in one single table.

We load the sample data and have a quick look

```{r}

exdata <- readRDS(system.file("extdata", "ex_data.rds", 
                            package = "ATdatabase"))
str(exdata)
print(summary(exdata))
exdata

```

## Downloading and storing  sensor info

Information about station are divided over two database tables. The
station table only contains the station identifier, the coordinates
(lat, lon) and a timestamp. All other station data is stored in the
meta table as unstructured data.

To get the generalised data from a station (identifier, lat, lon) we
first create a download function which does all the API calls. In our
example this is just a simple select from the example data. The
download function must return a data.frame with three fields:
'station', 'lat', and 'lon'. The station field is the station
identifier (a string), the lat and lon fields are the coordinates (as numeric
values).


```{r}

ex_download_station_info <- function(d = exdata) {
    res <- d %>%
        distinct(station, lat, lon)
    return(res)
}

stat_info <- ex_download_station_info()
stat_info

```

The station info can then be stored in the database. You only can
store one station at a time. So if you have a data frame with station
info as above, then you have to use either a for loop of an apply
function. In the follwoing example we get the station information and
use a vectorized function to store the data into the database using
the `insert_location_info` function from the package.

```{r}

insert_location_info_vectorized <- function(x, conn) {

    insert_location_info(station = x[1],
                        lat = as.numeric(x[2]),
                        lon = as.numeric(x[3]),
                        conn)

}

apply(stat_info, 1, FUN  = insert_location_info_vectorized, 
      conn = dbconn)

```

Now check how our location table looks like.

```{r}
s <- tbl(dbconn, "location") %>%
    head() %>%
    collect()
s

```

As you probably allready noticed, we didn't provide a timestamp. The
insert_location_info function adds the current timestamp to the record
if no timestamp is given.

### Adding meta data

Meta data can be stored in the database using the `add_doc` function.
Meta data can be any R object and is internally stored as JSON
document. Each meta data object must be given a type and reference
value. For example meta data about a station can be of type 'station'
while meta data about a sensor cab be of type 'sensor'. The reference
is a reference to, for example, the station id or the sensor type. As
user you can define any scheme of type and reference as you like.

The meta data object is just an R object, like a list, data.frame, or
vector.


As an example we create a meta data object containing the measured
parameters for each station. The we store the list of parameters into
the meta data, using 'station' as type and the station id as reference

We start to create the dataset.

```{r}

meta <- exdata %>% 
    na.omit() %>%
    group_by(station) %>% 
    summarise(pars = paste(unique(parameter), collapse = " "))
meta

```

Then we store a single object

```{r}

statname <- meta$station[1]
statpars <- meta$pars[1]

add_doc(type = "station",
        ref = statname,
        doc = statpars,
        conn = dbconn
        )



```

And then we get the some object from the database. This document is
identical to the object we stored in the database.

```{r}

doc <- get_doc(type = "station", ref = statname, conn = dbconn)
identical(statpars, doc)
```



## Downloading and storing measurements

* get data from example dataset (use download_data_example)
* hour averaged  
* example different time ranges  
* example using parameters/stations from meta  

# Use external download handler

* intro samanapir
* get sensor data + met stations  
* meta + data  
  

# Using the data

* use data from database  
* dbplyr pipes & collect  

# End
```{r}
# closing stuff


```



