---
title: "ATdatabase"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ATdatabase}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo = FALSE, messages = FALSE}
library(dplyr)
library(RSQLite)
library(pool)
library(ATdatabase)
```


# Introduction

The ATDatabase package provides a caching database for the Analyse Together
(AT) Tool. With this tool people can analyse the data from their own
environmental sensor, like citizens science air quality sensors. The
AT tool follows a modular design so that we can easily adapt the tool
for different projects or different user groups.  This effectively
will result in one generic tool and some other versions, more catered towards
specific usages.

Data from a single environmental sensor are usually send to data
providers who store the data and make the data available using API's
or websites. Data exploration tools like the AT tool also combine data
from several providers, for example sensor data from community portals,
wheater data from meteorolical institutes and reference data from national
monitoring networks. All these data can be accessed by API's but
collecting the data is often slow. So it helps to reuse data after it
is downloaded from the API.

With this package you can create a database in which all the data from
different sources can be downloaded and stored in a generalised way for applications
like the AT tool. The database provides caching functions, so when
data is downloaded it is immediatly available for other users (or even
applications)

While the sensor data is stored using a generalised data model, the
pacakge also provides in data base tables and methods to store any
other (unstructured) data. For example, information about type of
sensor of about the location can also stored using any format, as long
it is usable within R.

# Creating the database

Before we can create the database, we first have to look at the
database model.

## database model

The database model is based on the concept of mobile or static
stations. Each station contains equipment or sensors which measure a
certain parameter, and the values of these parameters change over time.

A station can be any assemblage of measuring equipment. For example it
can be a huge and expensive reference air monitoring station, but it
can also be a cheap hobyist ducktape assemblage of exotic electronics.
Each station contains one or more sensors.
Such sensor doesn't have to be an air quality monitoring
sensor, also meteorological sensors are possible. It can be anything
that measures something, somewhere, at some moment in time.

The variation of the measured values can be aggregated. If the sensor
measures a value each second, these values can be aggregated by
reporting the average of these values over one hour. For example, air
quality data is often reported as one hour averaged values, because
this gives a more robust and consistent result compared to the strong varying
real time data. It is up to the user to determine at which aggregation
level the data is stored in the database, the minimum is one value
each second.

While basic sensor information, like station name and position, and
measurements are stored as structured data, meta data about the station
or sensor (or project or region where they belong to) can be stored in any
format, as unstructured data.  For the storage of these meta data a NoSQL
approach is used.  In this approach meta data is stored as JSON documents.

The database uses four tables to store both measurements, caching
information and meta data. These four tables are:

* Measurements
* caching
* location  
* meta  

In the next sections we will explain the information model of the
database in detail.

### Measurements table and timestamps


The measurements table contains all the measurements

Table         | fields      |format  | description
--------------|-------------|--------|-------------
measurements  | id          | INT    | primary key
.             | station     | TEXT   | station id 
.             | parameter   | TEXT   | parameter / sensor
.             | value       | REAL   | measured value
.             | aggregation | REAL   | aggregation period in seconds
.             | timestamp   | INT    | date - time of observation


The station id is the name of the station, it is stored as a string.
Any name can be chosen. The parameter is the name of the measured
parameter stored as string. Addition information about the parameter,
like unit of sensor type, should be stored in the meta data.

The value, stored as 'real', is a numerical value of the measurement.
The aggregation field gives the aggregation level in seconds on which
the value is based on. 

Please note that you can not store values from the same station with
different aggregation levels. For example with air quality data, you
either have to chose between real time data or one hour averages. If
you want to use both, then collect the real time data and calculate
the one hour averages yourself.

The timestamp is the date and time of the observation. This is stored
in seconds! While odd at first to have these timestamps in seconds,
it is very practical to determine which time ranges are present in the
database and which time ranges should be added (this is what the
caching part of this package does). Seconds are also used internally
in the operating system, the 'start of time' (second 0) is the 'UNIX
epoch': 01-01-1970 at 00:00 UTC. The way we record timestamps is for
each table the same, so all timestamps in other tables are also in
seconds.

### Location table

The location table contains basic information about the station, like
name and position.

Table      | fields    |format   | description
-----------|-----------|---------|-------------
location   | id        | INT     | primary key
.          | station   | TEXT    | station id
.          | lat, lon  | REAL    | latitude / longitude
.          | timestamp | INT     | date - time of station info


The station id is refered by the measurements table and contains the
name of the station. The lat and lon fields contain the position in latitude and
longitude of the station. While we asume that you store this position
using the WGS84 coordinate system, any coordinate system can be used,
even grid based systems, as long as you name the coordinates lat and
lon.

The timestamp contains the date and time when the position of the
station is recorded. Static stations have a single record in this
table while mobile station can have multiple records. It is up to the
user to combine station mobility with measurements.

### Caching table

In the caching table the time ranges for which information about a
station is present in the database, is recorded.

 
Table    | fields      | format  | description
---------|-------------|---------|-------------------------
caching  | id          | INT     | primary key
.        | station     | TEXT    | station id
.        | start, end  | INT     | time range / timestamps


Again, station is refered by the other tables. Start and end are the
start and end of a time range for which measurements are available in
the measurements table.

### Meta table

The meta table contains unstructured data about stations, sensors, or
whatever the user likes to store. 


Table | fields | format | description
------|--------|--------|-------------
meta  | id     | INT    | primary key
.     | type   | TEXT   | type
.     | ref    | TEXT   | reference
.     | doc    | TEXT   | JSON doc


The type field determines the type of the meta data. For example
'station' for station info or 'project' for project info. It is up to
the user to define the type. The ref field is a reference. For example
if the type is 'station' the ref field can contain a station id to
refer to a station.

The doc field contains the meta data itself.  The user can store vectors, lists or
data.frames as meta data. Internally this meta data is converted to a
JSON object and stored in the doc field. When the data is retrieved
the user gets the data as R object (vector, list, etc).

Because meta data is stored as, e.g., a data.frame, it is easy to get
all the data of a certain type returned as single data.frames. These
data.frames can be combined, resulting in one large table. This table
can then be used further within the model or application.

While this looks complicated, it is actually a very versatile way to
store data as you will see in the examples further on in this
vignette.


## Create the database

The current version of this packages uses SQLite as database backend.
While we foresee that other database systems, like PostgreSQL, can be
used, it is not tested yet.

To create a database we have to create an SQLite database, these
databases are file based. For this vignette we create this database in
the tempory R directory.

```{r create_database1}

library(pool)
library(RSQLite)
library(ATdatabase)

fname_db <- tempfile()
dbconn <- pool::dbPool(drv = RSQLite::SQLite(), dbname = fname_db)

```

After creating the database we have to create the database tables.
We can use the `create_database_tables` for that. After the
creation we check if the tables exists

```{r create_database2}

create_database_tables(dbconn)
pool::dbListTables(dbconn)

```

# Downloading and storing data

Before you can download and store (cache) your downloaded data you
have to make a few preparations. In general you have to create
functions to get the data about stations, for the location info table.
Also, you have to create a download function to obtain the station
measurements. These functions have to return the data in a specifific
format so they can be used in conjunction with the functions from this
package. For each data provider, you have to write these functions
since they will be specific for the data provider's API.

This package contains example data and we simulate the download of
data b reading this example data. It is not the real thing but i
should give you understanding how this all works. The sample data is a
single file with all the information in one single table.

We load the sample data and have a quick look

```{r}

exdata <- readRDS(system.file("extdata", "ex_data.rds", 
                            package = "ATdatabase"))
str(exdata)
print(summary(exdata))
exdata

```

## Downloading and storing sensor info

Information about station are divided over two database tables. The
location table only contains the station identifier, the coordinates
(lat, lon) and a timestamp. All other station data is stored in the
meta table as unstructured data.

To get the generalised data from a station (identifier, lat, lon) we
first create a download function which does all the API calls. In our
example this is just a simple select from the example data. The
download function must return a data.frame with three fields:
'station', 'lat', and 'lon'. The station field is the station
identifier (a string), the lat and lon fields are the coordinates (as numeric
values).


```{r}

ex_download_station_info <- function(d = exdata) {
    res <- d %>%
        distinct(station, lat, lon)
    return(res)
}

stat_info <- ex_download_station_info()
stat_info

```

The station info can then be stored in the database. You only can
store one station at a time. So if you have a data frame with station
info as above, then you have to use either a for loop of an apply
function. In the follwoing example we get the station information and
use a vectorized function to store the data into the database using
the `insert_location_info` function from the package.

```{r}

insert_location_info_vectorized <- function(x, conn) {

    insert_location_info(station = x[1],
                        lat = as.numeric(x[2]),
                        lon = as.numeric(x[3]),
                        conn)

}

apply(stat_info, 1, FUN  = insert_location_info_vectorized, 
      conn = dbconn)

```

Now check how our location table looks like.

```{r}
s <- tbl(dbconn, "location") %>%
    head() %>%
    collect()
s

```

As you probably allready noticed, we didn't provide a timestamp. The
insert_location_info function adds the current timestamp to the record
if no timestamp is given.

### Adding meta data

Meta data can be stored in the database using the `add_doc` function.
Meta data can be any R object and is internally stored as JSON
document. Each meta data object must be given a type and reference
value. For example meta data about a station can be of type 'station'
while meta data about a sensor cab be of type 'sensor'. The reference
is a reference to, for example, the station id or the sensor type. As
user you can define any scheme of type and reference as you like.

The meta data object is just an R object, like a list, data.frame, or
vector. From our example dataset we have for each station a comma seperated list of the measured parameters.

```{r echo = FALSE, messages = FALSE}

meta <- exdata %>% 
    na.omit() %>%
    group_by(station) %>% 
    summarise( pars = paste(unique(parameter), collapse = ","))
meta

```

As an example we create a meta data object containing the measured
parameters for each station. We can store this list of parameters as
meta data, using 'parameters' as type and the station id as reference

The meta data looks like:

```{r}
meta
```

Then we store a single object

```{r}

statname <- meta$station[1]
statpars <- meta$pars[1]

add_doc(type = "parameter",
        ref = statname,
        doc = statpars,
        conn = dbconn
        )

```

And then we get the some object from the database. This document is
identical to the object we stored in the database.

```{r}

doc <- get_doc(type = "parameter", ref = statname, conn = dbconn)
identical(statpars, doc)

```

To add multiple meta data objects you can use an for loop or an apply
function, and call add_doc repeatedly

```{r echo = FALSE, messages = FALSE}
# store other data as well

for(i in 2:nrow(meta)) {

    statname <- meta$station[i]
    statpars <- meta$pars[i]

    add_doc(type = "parameter",
            ref = statname,
            doc = statpars,
            conn = dbconn
    )
}


```

### Adding information about stations

One of the things you probably need is a list of stations, grouped by
municpality, region, or project. This list of stations can be used as
selection criteria to get, for example, locations to draw on a map.
This list can also be used if you want to download a dataset for a
complete project or region.

This list of station can be stored as meta data to retreive when
necessary. In our case we store the list of stations as a simple
vector. Further on in this vignette we use this list to get the
measured data. We store the meta data using 'station' as type and
'example' as reference (in this case a project name).


```{r}

stations <- meta$station
stations


add_doc(type = "stations",
        ref = "example",
        doc = stations,
        conn = dbconn
        )

```

# Downloading and storing measurements

Since the ATdatabase is a caching database, it is assumed that there
is an original dataasource. Most often this datasource is a database
or a web API. To add measurements to the database, the data must first
be downloaded before it can be stored.

Since each data source has its own interface to request and deliver
data, it is not possible to create a single 'fit for all' download
function. So the download proces is split in two. First, the user must
write a download function, the download handler. Then second this
download handler must be provided to a generic download function wich
calls the handler, proces the data and store them to the database. 

The generic download functions (`download_data`) takes the staion id,
the time range to download, the name of the download handler function,
and the database object as arguments. When this function is called it
checks which time ranges are allready downloaded for that station and
which time ranges are lacking. This means that as an user you ask a
single time range, but that internally you have to download multiple
smaller time ranges. For each time range that must be downloaded, the
download handler is called.

The download handler takes the station id and a single time range and
translates this into an database or API call.
It doesn't matter how you write the download handler as long as you
keep to the following prerequisits:

 1. The arguments of the function must be timerange, station, and
    database connection
 2. The return of the function is a data.frame with measurements   

Let's start with the arguments of the function. The timeranges is a matrix
with a single row and two columns. The first column is the start time in
seconds (POSIX time) and the second column in the end time in seconds.
Using the `datetime_to_matrix`  function you can translate ordinary
dates into this matrix.

The station id is the reference to a station, this id depends on the
data source. The station ids in the database are thus the same ids as
in the datasource.

The conn argument is the database conenction object. This is needed if
you want to get, for example, meta data from the database. E.g. the
names of the parameters to download.

For each download operation it is up to you which parameters to
download. This parameter selection is part of the download handler.
The easiest way is to always download the same selection from each
data source. Think carefully about which parameters you selected. It
is not possible to change this after you have downloaded the data,
unless you clear the database and start al over.

The return of the function must return a data.frame with at least the
following fields:

 1. _station:_ the station id
 2. _parameter_: the measured parameter
 3. _value_: the measured value
 4. _timestamp_the timestamp of the measurements, in seconds (!)   
 5. _aggregation_ aggregation level (in seconds!)


If the returning data.frame contains more field, then they are simply
ignored when storing the data into the measurement table. The time
stamp must be given in seconds (POSIXct format). The aggregation is
also in seconds, so for one hour averaged measurements this is 3600.

## Download handler

For the sake of simplicity, for this vignette we use a file with
measurements instead of an database or API. Our download handler thus
reads from file instead from a remote.

Our download handler looks like this:

```{r}
download_data_fun
```

```{r echo = FALSE, messages = FALSE}

# get some dates to work with

statname <- exdata$station[1]

start_date <- exdata %>%
    filter(station == statname) %>%
    pull(timestamp) %>%
    min() %>%
    lubridate::as_datetime()

end_date <- exdata %>%
    filter(station == statname) %>%
    pull(timestamp) %>%
    max() %>%
    lubridate::as_datetime()

```


Just as example, we call this function directly to show the input
arguments and output. For station `r statname` we wil get the data for
the time range `r start_date` to `r end_date`. The download_data_fun
requires that we provide this timerange as matrix, with time in
seconds. We use the `datetime_to_matrix` function to convert our
timestamps


```{r}

statname
start_date
end_date

time_stamp <- datetime_to_matrix(start_date, end_date)

x <- download_data_fun(time_stamp, statname, dbconn)
x

```

Off course you don't want to just download the data, but also store
the measurements and update the available time ranges. This is done in
the `download_data` function. 

```{r}

result <- download_data(station = statname, Tstart = start_date,
              Tend = end_date, 
              fun = download_data_fun, conn = dbconn)
```

The download data function returns the retreived data as result. It is
up to the user to do something with it, but the data is allready
stored thus it can be ignored.

## Getting data from multiple stations

The download_data function only gets data from a single station. To
get multiple stations you can use a for loop or an apply function. In
this example we get all the measurements for each station and store
them in the database.


```{r}

stations_list <- get_doc("stations", "example", conn = dbconn)

result <- lapply(stations_list,  FUN = download_data, Tstart = start_date,
      Tend = end_date, fun = download_data_fun, conn = dbconn)

```

The result variable contains a list with all the downloaded datasets.
However, since all the data is allready stored we can simply ignore
the result.

# Using the data

If you want to use the location info or the measurements from the
database, you can select them directly. It is easy to create a such
selection of data using dplyr functions. For example to select all
the measurements for station `r statname`:

```{r}

library(dplyr)
result <- tbl(dbconn, "measurements") %>%
    filter(station == statname) %>%
    select(parameter, value, timestamp) %>%
    collect() %>%
    mutate(timestamp  = lubridate::as_datetime(timestamp))
result
```

Please not that you have to 'collect' the data, otherwise the result
will be a 'lazy query' instead of a tibble. This lazy query is
evaluated when the object is used, but this evaluation does not happen
always, giving funky results. A drawback of the collect function is
that this 'freezes' the data, if the data is updated in the database,
your 'result' object won't be updated (but is does get updated if
it's a lazy query, since the query is executed when you call the
object).

If you use mutate functions, then it is best to do them after the
collect function.

If you want to get location data, you can also use dplyr functions,
for example:

```{r}

result <- tbl(dbconn, "location") %>%
    filter(station == statname) %>%
    select(lat,lon) %>%
    collect()
result

```

If you want to get meta data from the database, then use the get_doc
function. A few examples of this function are shown above.

Stay away from the cache table. Let the package handle this table


```{r echo = FALSE, messages = FALSE}
# closing stuff
pool::poolClose(dbconn)

```



